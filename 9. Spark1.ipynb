{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfHxUeSW0Dc2Bx/9+OMeEP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLo_KAvjiAKc","executionInfo":{"status":"ok","timestamp":1741181987510,"user_tz":-330,"elapsed":9922,"user":{"displayName":"Uday Singh","userId":"02796870780303216464"}},"outputId":"0083f3d9-4bb0-4513-e8ea-5a9bfdf6276f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["pip install pyspark"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.context import SparkContext"],"metadata":{"id":"-9xfoDMQiSZI","executionInfo":{"status":"ok","timestamp":1741182027438,"user_tz":-330,"elapsed":30,"user":{"displayName":"Uday Singh","userId":"02796870780303216464"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["spark = SparkSession.builder.appName('myapp').getOrCreate()"],"metadata":{"id":"G1CbY7uyiYo_","executionInfo":{"status":"ok","timestamp":1741182061626,"user_tz":-330,"elapsed":9300,"user":{"displayName":"Uday Singh","userId":"02796870780303216464"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["sc = spark.sparkContext"],"metadata":{"id":"xKzYJy0piaZi","executionInfo":{"status":"ok","timestamp":1741182061659,"user_tz":-330,"elapsed":27,"user":{"displayName":"Uday Singh","userId":"02796870780303216464"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["rdd = sc.parallelize([1,2,3,4,5])"],"metadata":{"id":"jwdVE32Aibdl","executionInfo":{"status":"ok","timestamp":1741182062083,"user_tz":-330,"elapsed":417,"user":{"displayName":"Uday Singh","userId":"02796870780303216464"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["rdd_squared = rdd.map(lambda x:x*x)"],"metadata":{"id":"FDHf7-2miccV","executionInfo":{"status":"ok","timestamp":1741182066789,"user_tz":-330,"elapsed":54,"user":{"displayName":"Uday Singh","userId":"02796870780303216464"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["rdd_squared.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"seShgzzgid7T","executionInfo":{"status":"ok","timestamp":1741182073578,"user_tz":-330,"elapsed":3181,"user":{"displayName":"Uday Singh","userId":"02796870780303216464"}},"outputId":"57685fe2-58a6-4871-cfde-d9db7ca35a08"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"owW9xUP6ie0K","executionInfo":{"status":"ok","timestamp":1741182080541,"user_tz":-330,"elapsed":419,"user":{"displayName":"Uday Singh","userId":"02796870780303216464"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["- In pysaprk the driver program and spark context play an imortant role\n","- Spark context is the entry point to sparks core functionalities before spark 2.0 it was used directrly ,but after spark 2.0 a higher level api sparksession is the new entry point that internally contains spark context.\n","- In the above code when we run the script the driver program starts and creates spark Context it builds a DAG of transformations but when an action in this case collect is called spark executes the transformations.\n","\n","\n"],"metadata":{"id":"Y3tAo5-gjhAe"}},{"cell_type":"code","source":[],"metadata":{"id":"Cv51CUdQihLt"},"execution_count":null,"outputs":[]}]}